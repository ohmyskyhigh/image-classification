{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [00:13, 12.8MB/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe7625aee10>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    normalized = (255 - x) / 255\n",
    "    return x / np.max(x, axis = 0)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    rows = len(x)      \n",
    "    onehot_x = np.zeros([rows,10])\n",
    "\n",
    "    for idx,v in enumerate(x):      \n",
    "        onehot_x[idx][v] = 1\n",
    "        \n",
    "    ##using sklearn\n",
    "    ##lb = preprocessing.LabelBinarizer()\n",
    "    ##lb.fit(x)\n",
    "    ##\n",
    "    return onehot_x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, shape=(None, *image_shape), name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=[None, n_classes], name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d(x, W, b, strides, p='SAME'):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides[0], strides[1], 1], padding=p)\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, ksize, stride, p='SAME'):\n",
    "    return tf.nn.max_pool(x, ksize=[1, ksize[0], ksize[1], 1], strides=[1, stride[0], stride[1], 1], padding=p)\n",
    "\n",
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of output filters for the convolutional layer (new depth)\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #tensor shape\n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "    shape = shape[3]\n",
    "    weights = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1], shape, conv_num_outputs], mean=0, \n",
    "                                              stddev=0.05))\n",
    "    bias = tf.Variable(tf.truncated_normal([conv_num_outputs], mean=0, stddev=0.05))\n",
    "    conv = conv2d(x_tensor, weights, bias, conv_strides)\n",
    "    conv = maxpool2d(conv, pool_ksize, pool_strides)\n",
    "    return conv\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 9,2])\n",
    "    shape = x_tensor.get_shape().as_list()        # a list: [None, 9, 2]\n",
    "    dim = numpy.prod(shape[1:])            # dim = prod(9,2) = 18\n",
    "    x2 = tf.reshape(x, [-1, dim])           # -1 means \"all\n",
    "    \n",
    "    return tf.contrib.layers.flatten(x_tensor) \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    ###Neurons in a fully connected \n",
    "    ##layer have full connections to all activations in the previous layer, \n",
    "    ##as seen in regular Neural Networks.\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    return tf.contrib.layers.fully_connected(inputs=x_tensor, num_outputs=num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs, activation_fn=None)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 8, 8, 64)\n",
      "(?, 4, 4, 128)\n",
      "(?, 1, 1, 150)\n",
      "(?, 8, 8, 64)\n",
      "(?, 4, 4, 128)\n",
      "(?, 1, 1, 150)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    conv_num_outputs = 64\n",
    "    conv_ksize = (5, 5)\n",
    "    conv_strides = (2, 2)\n",
    "    pool_ksize = (3, 3)\n",
    "    pool_strides = (2, 2)\n",
    "    # Function Definition from Above:\n",
    "    conv1 = conv2d_maxpool(x, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    print(conv1.get_shape())\n",
    "    conv1 = tf.nn.dropout(conv1, keep_prob)    \n",
    "    conv2 = conv2d_maxpool(conv1, 128, (2, 2), (1, 1), (2, 2), (2, 2))\n",
    "    conv2 = tf.nn.dropout(conv2, keep_prob)\n",
    "    conv3 = conv2d_maxpool(conv2, 150, (4, 4), (4, 4), (1, 1), (1, 1))\n",
    "    print(conv2.get_shape())\n",
    "    print(conv3.get_shape())\n",
    "\n",
    "    \n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    flatten_in = flatten(conv3)\n",
    "    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    num_outputs = 200\n",
    "    # Function Definition from Above:\n",
    "    fullyN = fully_conn(flatten_in, num_outputs)\n",
    "    fullyN = tf.nn.dropout(fullyN, keep_prob)\n",
    "    fullyN = fully_conn(fullyN, 100)\n",
    "\n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    #because cifar-10, maybe\n",
    "    num_outputs = 10\n",
    "    # Function Definition from Above:\n",
    "    out = output(fullyN, num_outputs)\n",
    "    \n",
    "    \n",
    "    # TODO: return output\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    return session.run(optimizer, feed_dict={x: feature_batch,\n",
    "                                             y: label_batch,\n",
    "                                             keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={x: feature_batch,\n",
    "                                       y: label_batch,\n",
    "                                       keep_prob: 1.})\n",
    "    valid_acc = session.run(accuracy, feed_dict={x: valid_features,\n",
    "                                                y: valid_labels,\n",
    "                                                keep_prob: 1.})\n",
    "    print('Loss: {:>10.4f} Accuracy: {:.6f}'.format(loss,valid_acc))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.1511 Accuracy: 0.261600\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     1.9426 Accuracy: 0.338000\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.7988 Accuracy: 0.408600\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.6896 Accuracy: 0.432000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.6371 Accuracy: 0.465800\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.5367 Accuracy: 0.466200\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.4940 Accuracy: 0.483800\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.3823 Accuracy: 0.499800\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.3701 Accuracy: 0.506400\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.2580 Accuracy: 0.524200\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.2697 Accuracy: 0.527600\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.2440 Accuracy: 0.534000\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.1799 Accuracy: 0.540800\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.0935 Accuracy: 0.547200\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.0517 Accuracy: 0.558600\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.0670 Accuracy: 0.557600\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     0.9755 Accuracy: 0.559200\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     0.9485 Accuracy: 0.567400\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     0.9153 Accuracy: 0.580800\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     0.8897 Accuracy: 0.583400\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     0.8991 Accuracy: 0.589400\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     0.8453 Accuracy: 0.575600\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     0.8015 Accuracy: 0.579600\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     0.8113 Accuracy: 0.581000\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     0.7937 Accuracy: 0.594800\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     0.8160 Accuracy: 0.596200\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     0.7564 Accuracy: 0.603600\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     0.7485 Accuracy: 0.610400\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     0.7405 Accuracy: 0.606200\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     0.6791 Accuracy: 0.615600\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     0.6439 Accuracy: 0.609600\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     0.6141 Accuracy: 0.614200\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     0.6482 Accuracy: 0.609200\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     0.6484 Accuracy: 0.612200\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     0.5597 Accuracy: 0.624600\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     0.5872 Accuracy: 0.611600\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     0.5461 Accuracy: 0.613400\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     0.4915 Accuracy: 0.610800\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     0.4727 Accuracy: 0.620200\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     0.4720 Accuracy: 0.621800\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     0.4401 Accuracy: 0.624000\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     0.4626 Accuracy: 0.627800\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     0.4426 Accuracy: 0.623200\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     0.3697 Accuracy: 0.629200\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     0.3566 Accuracy: 0.631000\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.3781 Accuracy: 0.630200\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.4380 Accuracy: 0.628600\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.3764 Accuracy: 0.627000\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.3999 Accuracy: 0.630600\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.3299 Accuracy: 0.630400\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.3250 Accuracy: 0.629800\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.3335 Accuracy: 0.634200\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.2901 Accuracy: 0.625400\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.2869 Accuracy: 0.627400\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.2757 Accuracy: 0.624600\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.3132 Accuracy: 0.632600\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.2821 Accuracy: 0.637800\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.2904 Accuracy: 0.625000\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.2662 Accuracy: 0.631600\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.2049 Accuracy: 0.629200\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.2414 Accuracy: 0.630800\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.2159 Accuracy: 0.629200\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.2310 Accuracy: 0.630400\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.2218 Accuracy: 0.633800\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.2438 Accuracy: 0.626400\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.1943 Accuracy: 0.629600\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.2018 Accuracy: 0.626000\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.1777 Accuracy: 0.638000\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.1539 Accuracy: 0.635600\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.1731 Accuracy: 0.632200\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.1424 Accuracy: 0.632400\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.1558 Accuracy: 0.633000\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.1375 Accuracy: 0.631800\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.1480 Accuracy: 0.631800\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.1600 Accuracy: 0.630000\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.1599 Accuracy: 0.624400\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.1603 Accuracy: 0.628200\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.1152 Accuracy: 0.639800\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.1642 Accuracy: 0.630000\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.1278 Accuracy: 0.632600\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.1121 Accuracy: 0.630400\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.1278 Accuracy: 0.627000\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.1589 Accuracy: 0.614600\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.1463 Accuracy: 0.619600\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.0898 Accuracy: 0.629400\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.1029 Accuracy: 0.625600\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.1110 Accuracy: 0.627800\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.0842 Accuracy: 0.639200\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.0881 Accuracy: 0.636400\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.0923 Accuracy: 0.627800\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.1353 Accuracy: 0.626000\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.1786 Accuracy: 0.623400\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.0811 Accuracy: 0.640400\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.0642 Accuracy: 0.642200\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.0770 Accuracy: 0.629600\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.0978 Accuracy: 0.630400\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.1080 Accuracy: 0.636600\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.0732 Accuracy: 0.625600\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.0719 Accuracy: 0.631000\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.0638 Accuracy: 0.636200\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.1952 Accuracy: 0.227400\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     1.8604 Accuracy: 0.300400\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     1.6335 Accuracy: 0.389200\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     1.7256 Accuracy: 0.409200\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     1.6795 Accuracy: 0.428400\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     1.7801 Accuracy: 0.455000\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     1.5634 Accuracy: 0.464400\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     1.3632 Accuracy: 0.469600\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     1.4705 Accuracy: 0.489800\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     1.5015 Accuracy: 0.486000\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.6109 Accuracy: 0.487000\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     1.3769 Accuracy: 0.501000\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     1.2328 Accuracy: 0.514400\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     1.3724 Accuracy: 0.544600\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     1.4305 Accuracy: 0.506000\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.3843 Accuracy: 0.539400\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     1.3077 Accuracy: 0.549600\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     1.1958 Accuracy: 0.543800\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     1.2758 Accuracy: 0.557400\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     1.2652 Accuracy: 0.544200\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.2672 Accuracy: 0.570200\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     1.2557 Accuracy: 0.566600\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     1.0803 Accuracy: 0.578600\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     1.1638 Accuracy: 0.564400\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     1.1992 Accuracy: 0.570200\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.1425 Accuracy: 0.600200\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     1.1800 Accuracy: 0.596200\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     1.0252 Accuracy: 0.587600\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     1.1026 Accuracy: 0.608400\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     1.2484 Accuracy: 0.557800\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.1041 Accuracy: 0.607800\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     1.0697 Accuracy: 0.606800\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     0.9526 Accuracy: 0.610200\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     1.0203 Accuracy: 0.605400\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     1.0595 Accuracy: 0.615800\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.0189 Accuracy: 0.626600\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     1.0806 Accuracy: 0.617800\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     0.8755 Accuracy: 0.613400\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     0.9939 Accuracy: 0.625800\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     1.0354 Accuracy: 0.612400\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     0.9893 Accuracy: 0.624200\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     1.0800 Accuracy: 0.618200\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     0.8274 Accuracy: 0.631200\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     0.9628 Accuracy: 0.642200\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     0.9862 Accuracy: 0.625200\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     0.9783 Accuracy: 0.630800\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     1.0073 Accuracy: 0.630200\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     0.8849 Accuracy: 0.633800\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     0.8906 Accuracy: 0.647400\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     0.9894 Accuracy: 0.635800\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     0.8975 Accuracy: 0.642000\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     0.9226 Accuracy: 0.654000\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     0.8101 Accuracy: 0.646400\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     0.9106 Accuracy: 0.651000\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     0.9138 Accuracy: 0.630600\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     0.9171 Accuracy: 0.654600\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     0.9557 Accuracy: 0.651600\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     0.7732 Accuracy: 0.655400\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     0.7882 Accuracy: 0.662800\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     0.9352 Accuracy: 0.623200\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     0.9034 Accuracy: 0.662000\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     0.8794 Accuracy: 0.653800\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     0.7769 Accuracy: 0.652200\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     0.7369 Accuracy: 0.662200\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     0.8489 Accuracy: 0.661400\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     0.8577 Accuracy: 0.656600\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     0.9057 Accuracy: 0.665600\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     0.7033 Accuracy: 0.668800\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     0.7449 Accuracy: 0.674400\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     0.8411 Accuracy: 0.660600\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     0.7555 Accuracy: 0.670400\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     0.8708 Accuracy: 0.671800\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     0.6056 Accuracy: 0.673400\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     0.7203 Accuracy: 0.668600\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     0.7410 Accuracy: 0.667800\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     0.7159 Accuracy: 0.683400\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     0.8160 Accuracy: 0.683800\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     0.6369 Accuracy: 0.662400\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     0.6523 Accuracy: 0.685600\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     0.7233 Accuracy: 0.673600\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     0.8035 Accuracy: 0.679600\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     0.8619 Accuracy: 0.683200\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     0.6037 Accuracy: 0.688600\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     0.6272 Accuracy: 0.680200\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     0.7025 Accuracy: 0.689600\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     0.7402 Accuracy: 0.675400\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     0.7482 Accuracy: 0.676600\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     0.5473 Accuracy: 0.675600\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     0.6422 Accuracy: 0.687600\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     0.6798 Accuracy: 0.676000\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     0.7100 Accuracy: 0.691000\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     0.7575 Accuracy: 0.686600\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     0.5032 Accuracy: 0.688400\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     0.6550 Accuracy: 0.688000\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     0.6635 Accuracy: 0.675200\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     0.7376 Accuracy: 0.695200\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     0.7265 Accuracy: 0.697800\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     0.5306 Accuracy: 0.698000\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     0.5912 Accuracy: 0.698400\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     0.6589 Accuracy: 0.696600\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     0.6445 Accuracy: 0.695200\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:     0.6930 Accuracy: 0.697200\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:     0.4989 Accuracy: 0.686600\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:     0.6116 Accuracy: 0.692800\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:     0.6292 Accuracy: 0.690200\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     0.6425 Accuracy: 0.699200\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:     0.6391 Accuracy: 0.695400\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:     0.5447 Accuracy: 0.683600\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:     0.5731 Accuracy: 0.699800\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:     0.6054 Accuracy: 0.699400\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     0.5726 Accuracy: 0.706800\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:     0.6566 Accuracy: 0.707400\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:     0.4755 Accuracy: 0.698200\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:     0.5058 Accuracy: 0.706200\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:     0.5623 Accuracy: 0.695200\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     0.6023 Accuracy: 0.703800\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:     0.6355 Accuracy: 0.707800\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:     0.4925 Accuracy: 0.689600\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:     0.5177 Accuracy: 0.698800\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:     0.5622 Accuracy: 0.695200\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     0.6241 Accuracy: 0.705200\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:     0.6308 Accuracy: 0.698000\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:     0.4930 Accuracy: 0.703400\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:     0.5129 Accuracy: 0.704200\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:     0.5597 Accuracy: 0.706600\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     0.5935 Accuracy: 0.713000\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:     0.6215 Accuracy: 0.700600\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:     0.4143 Accuracy: 0.702400\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:     0.4884 Accuracy: 0.708400\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:     0.5683 Accuracy: 0.693800\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     0.6314 Accuracy: 0.715200\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:     0.6639 Accuracy: 0.709000\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:     0.4509 Accuracy: 0.713400\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:     0.4646 Accuracy: 0.713600\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:     0.5557 Accuracy: 0.705000\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     0.6307 Accuracy: 0.714000\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:     0.6426 Accuracy: 0.717200\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:     0.4487 Accuracy: 0.705400\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:     0.4044 Accuracy: 0.710800\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:     0.4855 Accuracy: 0.708200\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     0.5638 Accuracy: 0.708200\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:     0.6055 Accuracy: 0.699800\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:     0.3805 Accuracy: 0.714600\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:     0.4185 Accuracy: 0.720800\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:     0.4471 Accuracy: 0.710000\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     0.6055 Accuracy: 0.705400\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:     0.6117 Accuracy: 0.707600\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:     0.4003 Accuracy: 0.711600\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:     0.4294 Accuracy: 0.709400\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:     0.4506 Accuracy: 0.719400\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     0.5607 Accuracy: 0.711200\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:     0.5851 Accuracy: 0.718200\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:     0.3749 Accuracy: 0.719600\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:     0.4510 Accuracy: 0.719200\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:     0.4908 Accuracy: 0.721400\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     0.6080 Accuracy: 0.716000\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:     0.5966 Accuracy: 0.720200\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:     0.4093 Accuracy: 0.718000\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:     0.4611 Accuracy: 0.710400\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:     0.4725 Accuracy: 0.716600\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     0.6255 Accuracy: 0.704800\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:     0.5669 Accuracy: 0.705000\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:     0.3789 Accuracy: 0.716200\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:     0.3993 Accuracy: 0.720800\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:     0.4450 Accuracy: 0.714200\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     0.6317 Accuracy: 0.723600\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:     0.6317 Accuracy: 0.702200\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:     0.3553 Accuracy: 0.715400\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:     0.3940 Accuracy: 0.723600\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:     0.3982 Accuracy: 0.723800\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     0.5698 Accuracy: 0.724000\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:     0.6160 Accuracy: 0.708000\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:     0.3718 Accuracy: 0.716800\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:     0.3665 Accuracy: 0.732400\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:     0.3974 Accuracy: 0.718600\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     0.5819 Accuracy: 0.719800\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:     0.5288 Accuracy: 0.713600\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:     0.3626 Accuracy: 0.715200\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:     0.3734 Accuracy: 0.726800\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:     0.4360 Accuracy: 0.726000\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     0.5488 Accuracy: 0.725600\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:     0.5094 Accuracy: 0.713600\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:     0.4012 Accuracy: 0.720000\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:     0.3610 Accuracy: 0.725000\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:     0.4070 Accuracy: 0.725400\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     0.5299 Accuracy: 0.726000\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:     0.5657 Accuracy: 0.714600\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:     0.3974 Accuracy: 0.719800\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:     0.3112 Accuracy: 0.731800\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:     0.4063 Accuracy: 0.725200\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     0.4814 Accuracy: 0.724400\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:     0.5143 Accuracy: 0.718000\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:     0.3522 Accuracy: 0.724600\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:     0.3121 Accuracy: 0.729600\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:     0.4432 Accuracy: 0.726600\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     0.5538 Accuracy: 0.731400\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:     0.4845 Accuracy: 0.721800\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:     0.3852 Accuracy: 0.725200\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:     0.3520 Accuracy: 0.725400\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:     0.4231 Accuracy: 0.723800\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     0.4816 Accuracy: 0.729200\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:     0.5223 Accuracy: 0.717200\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:     0.3126 Accuracy: 0.731400\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:     0.3341 Accuracy: 0.734800\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:     0.4183 Accuracy: 0.722800\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     0.5048 Accuracy: 0.728000\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:     0.5328 Accuracy: 0.724600\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:     0.3787 Accuracy: 0.722400\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:     0.3620 Accuracy: 0.723400\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:     0.3883 Accuracy: 0.722800\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     0.5523 Accuracy: 0.721600\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:     0.5102 Accuracy: 0.721000\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:     0.3254 Accuracy: 0.723600\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:     0.2963 Accuracy: 0.732400\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:     0.3603 Accuracy: 0.728400\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     0.5056 Accuracy: 0.723800\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:     0.5149 Accuracy: 0.721600\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:     0.3197 Accuracy: 0.724800\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:     0.3208 Accuracy: 0.734600\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:     0.3406 Accuracy: 0.725200\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     0.5161 Accuracy: 0.727200\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:     0.5514 Accuracy: 0.728400\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:     0.3140 Accuracy: 0.728200\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:     0.3776 Accuracy: 0.723800\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:     0.2781 Accuracy: 0.732800\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.5032 Accuracy: 0.722600\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:     0.4941 Accuracy: 0.711800\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:     0.3514 Accuracy: 0.727600\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:     0.3137 Accuracy: 0.736000\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:     0.3355 Accuracy: 0.713400\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.4942 Accuracy: 0.727400\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:     0.4398 Accuracy: 0.727200\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:     0.3295 Accuracy: 0.729200\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:     0.3294 Accuracy: 0.734600\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:     0.3254 Accuracy: 0.723800\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.4946 Accuracy: 0.730800\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:     0.4290 Accuracy: 0.720200\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:     0.2753 Accuracy: 0.733800\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:     0.2896 Accuracy: 0.734600\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:     0.3554 Accuracy: 0.724800\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.4981 Accuracy: 0.727600\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:     0.4451 Accuracy: 0.721400\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:     0.2855 Accuracy: 0.733200\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:     0.2838 Accuracy: 0.728000\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:     0.3261 Accuracy: 0.731000\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.4854 Accuracy: 0.729200\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:     0.4500 Accuracy: 0.722400\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:     0.2731 Accuracy: 0.733600\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:     0.3095 Accuracy: 0.735800\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:     0.3082 Accuracy: 0.725800\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.4725 Accuracy: 0.730000\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:     0.4536 Accuracy: 0.717200\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:     0.2589 Accuracy: 0.737800\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:     0.2788 Accuracy: 0.737400\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:     0.3092 Accuracy: 0.730000\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.5120 Accuracy: 0.723200\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:     0.4985 Accuracy: 0.733000\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:     0.2738 Accuracy: 0.734600\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:     0.2752 Accuracy: 0.737200\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:     0.2870 Accuracy: 0.735200\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.4856 Accuracy: 0.730800\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:     0.4474 Accuracy: 0.727000\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:     0.2893 Accuracy: 0.730800\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:     0.2884 Accuracy: 0.735200\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:     0.3117 Accuracy: 0.729800\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.4258 Accuracy: 0.745800\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:     0.4541 Accuracy: 0.739200\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:     0.2510 Accuracy: 0.740200\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:     0.2828 Accuracy: 0.743000\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:     0.3020 Accuracy: 0.734800\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.4391 Accuracy: 0.722600\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:     0.4832 Accuracy: 0.730000\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:     0.2441 Accuracy: 0.739600\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:     0.2983 Accuracy: 0.745200\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:     0.3047 Accuracy: 0.731200\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.4435 Accuracy: 0.738400\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:     0.4868 Accuracy: 0.729200\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:     0.2584 Accuracy: 0.735600\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:     0.2990 Accuracy: 0.733400\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:     0.3304 Accuracy: 0.724000\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.4333 Accuracy: 0.734800\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:     0.4143 Accuracy: 0.732000\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:     0.2419 Accuracy: 0.745800\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:     0.3105 Accuracy: 0.735400\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:     0.3212 Accuracy: 0.731600\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.4577 Accuracy: 0.737200\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:     0.4413 Accuracy: 0.733000\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:     0.2422 Accuracy: 0.735000\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:     0.2945 Accuracy: 0.737800\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:     0.3038 Accuracy: 0.740600\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.4590 Accuracy: 0.721000\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:     0.4066 Accuracy: 0.741000\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:     0.2450 Accuracy: 0.742000\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:     0.3034 Accuracy: 0.743400\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:     0.2972 Accuracy: 0.734600\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.4224 Accuracy: 0.734200\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:     0.4318 Accuracy: 0.728000\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:     0.2829 Accuracy: 0.735000\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:     0.2763 Accuracy: 0.743600\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:     0.2961 Accuracy: 0.743800\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.4271 Accuracy: 0.733800\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:     0.3941 Accuracy: 0.736200\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:     0.2810 Accuracy: 0.731600\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:     0.2971 Accuracy: 0.742200\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:     0.3023 Accuracy: 0.741600\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.4372 Accuracy: 0.737200\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:     0.3951 Accuracy: 0.739800\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:     0.2697 Accuracy: 0.743200\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:     0.2814 Accuracy: 0.745200\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:     0.3176 Accuracy: 0.729600\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.4265 Accuracy: 0.744200\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:     0.4064 Accuracy: 0.731600\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:     0.2370 Accuracy: 0.743200\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:     0.2545 Accuracy: 0.744200\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:     0.2970 Accuracy: 0.732200\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.4481 Accuracy: 0.739000\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:     0.4437 Accuracy: 0.735200\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:     0.2446 Accuracy: 0.744400\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:     0.2730 Accuracy: 0.749400\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:     0.2763 Accuracy: 0.741600\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.3835 Accuracy: 0.733400\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:     0.4196 Accuracy: 0.736000\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:     0.2269 Accuracy: 0.746200\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:     0.3000 Accuracy: 0.743600\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss:     0.2872 Accuracy: 0.732800\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.3859 Accuracy: 0.739000\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:     0.3812 Accuracy: 0.737000\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:     0.2378 Accuracy: 0.747000\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:     0.2436 Accuracy: 0.746200\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:     0.3140 Accuracy: 0.738000\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.4067 Accuracy: 0.742200\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:     0.3565 Accuracy: 0.741600\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:     0.2399 Accuracy: 0.730200\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:     0.2730 Accuracy: 0.743200\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:     0.3330 Accuracy: 0.732400\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.3797 Accuracy: 0.735000\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:     0.4007 Accuracy: 0.732800\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:     0.2554 Accuracy: 0.741800\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:     0.2462 Accuracy: 0.739800\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:     0.2777 Accuracy: 0.743200\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.4129 Accuracy: 0.745800\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:     0.4168 Accuracy: 0.736600\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:     0.2324 Accuracy: 0.749600\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:     0.2592 Accuracy: 0.744000\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:     0.2582 Accuracy: 0.733600\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.3534 Accuracy: 0.748000\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:     0.3717 Accuracy: 0.736200\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:     0.2347 Accuracy: 0.745400\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:     0.2681 Accuracy: 0.744800\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:     0.2792 Accuracy: 0.742000\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.3711 Accuracy: 0.741800\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:     0.4017 Accuracy: 0.736200\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:     0.2457 Accuracy: 0.740800\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:     0.2791 Accuracy: 0.740200\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:     0.2716 Accuracy: 0.743400\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.3567 Accuracy: 0.747600\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:     0.3889 Accuracy: 0.739400\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:     0.2157 Accuracy: 0.749600\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:     0.2722 Accuracy: 0.747200\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:     0.2721 Accuracy: 0.740000\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.4429 Accuracy: 0.741000\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:     0.3530 Accuracy: 0.741200\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:     0.1969 Accuracy: 0.747600\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:     0.2585 Accuracy: 0.756200\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:     0.2746 Accuracy: 0.748400\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.3735 Accuracy: 0.744000\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:     0.3886 Accuracy: 0.736200\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:     0.2384 Accuracy: 0.747600\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:     0.2788 Accuracy: 0.742200\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:     0.2726 Accuracy: 0.742400\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.3274 Accuracy: 0.746200\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:     0.3650 Accuracy: 0.734200\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:     0.2068 Accuracy: 0.746000\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:     0.2775 Accuracy: 0.739000\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:     0.2611 Accuracy: 0.740200\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.3599 Accuracy: 0.741800\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:     0.3665 Accuracy: 0.740800\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:     0.1945 Accuracy: 0.743600\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:     0.2740 Accuracy: 0.753400\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:     0.2591 Accuracy: 0.739600\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.3437 Accuracy: 0.740200\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:     0.3306 Accuracy: 0.741800\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:     0.2371 Accuracy: 0.746000\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:     0.2427 Accuracy: 0.748000\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:     0.2600 Accuracy: 0.742200\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.3689 Accuracy: 0.744600\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:     0.3627 Accuracy: 0.732200\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:     0.2061 Accuracy: 0.751400\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:     0.2587 Accuracy: 0.740800\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:     0.2615 Accuracy: 0.747400\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.3611 Accuracy: 0.730600\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:     0.3977 Accuracy: 0.733600\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:     0.2005 Accuracy: 0.748200\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:     0.2544 Accuracy: 0.747200\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:     0.2776 Accuracy: 0.740400\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.3598 Accuracy: 0.742800\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:     0.3056 Accuracy: 0.746400\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:     0.2314 Accuracy: 0.743400\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:     0.3036 Accuracy: 0.740200\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:     0.2550 Accuracy: 0.748600\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.3679 Accuracy: 0.747400\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:     0.3472 Accuracy: 0.743800\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:     0.1936 Accuracy: 0.749600\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:     0.2696 Accuracy: 0.760600\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:     0.2631 Accuracy: 0.737600\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.3274 Accuracy: 0.745200\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:     0.4168 Accuracy: 0.737200\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:     0.1991 Accuracy: 0.739000\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:     0.2421 Accuracy: 0.750000\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:     0.2125 Accuracy: 0.749200\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.3484 Accuracy: 0.746400\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:     0.3426 Accuracy: 0.740800\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:     0.2212 Accuracy: 0.744200\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:     0.2400 Accuracy: 0.749600\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:     0.2419 Accuracy: 0.744800\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.3678 Accuracy: 0.738600\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:     0.3383 Accuracy: 0.740400\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:     0.2012 Accuracy: 0.748200\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:     0.2376 Accuracy: 0.748800\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:     0.2357 Accuracy: 0.738600\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.3181 Accuracy: 0.750200\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:     0.3626 Accuracy: 0.747000\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:     0.1700 Accuracy: 0.746600\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:     0.2293 Accuracy: 0.751600\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:     0.2623 Accuracy: 0.744400\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.3714 Accuracy: 0.739600\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:     0.3295 Accuracy: 0.740200\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:     0.1893 Accuracy: 0.755400\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss:     0.2548 Accuracy: 0.750000\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:     0.2508 Accuracy: 0.737800\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.3209 Accuracy: 0.741400\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:     0.3446 Accuracy: 0.739600\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss:     0.1853 Accuracy: 0.748200\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:     0.2803 Accuracy: 0.747400\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:     0.2491 Accuracy: 0.729800\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.3137 Accuracy: 0.743400\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:     0.3116 Accuracy: 0.735800\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:     0.1823 Accuracy: 0.754400\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:     0.2694 Accuracy: 0.744400\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:     0.2301 Accuracy: 0.736200\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.3830 Accuracy: 0.738200\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:     0.2873 Accuracy: 0.745200\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:     0.1865 Accuracy: 0.753800\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:     0.2545 Accuracy: 0.756400\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:     0.2562 Accuracy: 0.739000\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.3400 Accuracy: 0.741400\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:     0.2947 Accuracy: 0.745400\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:     0.1712 Accuracy: 0.749600\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:     0.2612 Accuracy: 0.744800\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:     0.2284 Accuracy: 0.740800\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.3479 Accuracy: 0.737200\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:     0.3166 Accuracy: 0.734000\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:     0.1786 Accuracy: 0.756200\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:     0.2397 Accuracy: 0.743600\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:     0.2443 Accuracy: 0.740600\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.4040 Accuracy: 0.733600\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:     0.3238 Accuracy: 0.742600\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:     0.1923 Accuracy: 0.750000\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:     0.2500 Accuracy: 0.750400\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:     0.2194 Accuracy: 0.750400\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.3783 Accuracy: 0.736200\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:     0.2958 Accuracy: 0.748000\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:     0.2010 Accuracy: 0.753800\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:     0.2017 Accuracy: 0.755800\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:     0.2297 Accuracy: 0.740200\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.3849 Accuracy: 0.733400\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:     0.3064 Accuracy: 0.747800\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:     0.2093 Accuracy: 0.747800\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:     0.1798 Accuracy: 0.751600\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:     0.2392 Accuracy: 0.744600\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.3147 Accuracy: 0.745400\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:     0.2936 Accuracy: 0.744400\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:     0.2084 Accuracy: 0.745600\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:     0.2091 Accuracy: 0.756200\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:     0.2198 Accuracy: 0.742400\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.3441 Accuracy: 0.734000\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:     0.2995 Accuracy: 0.734800\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:     0.2082 Accuracy: 0.750600\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:     0.2204 Accuracy: 0.750200\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:     0.2557 Accuracy: 0.740600\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.3498 Accuracy: 0.746200\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:     0.2867 Accuracy: 0.754400\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:     0.2137 Accuracy: 0.752600\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:     0.2258 Accuracy: 0.749400\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:     0.2527 Accuracy: 0.735600\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.3267 Accuracy: 0.729600\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:     0.3558 Accuracy: 0.742800\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:     0.1918 Accuracy: 0.748600\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:     0.1991 Accuracy: 0.753000\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:     0.2224 Accuracy: 0.737600\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.3353 Accuracy: 0.741600\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:     0.3119 Accuracy: 0.739000\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:     0.1934 Accuracy: 0.734400\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:     0.2054 Accuracy: 0.751200\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:     0.2015 Accuracy: 0.738600\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.2979 Accuracy: 0.733400\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:     0.3176 Accuracy: 0.747400\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:     0.1801 Accuracy: 0.753000\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:     0.1833 Accuracy: 0.752200\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:     0.1962 Accuracy: 0.742400\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.7378362341772152\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERGJtDgWEREREYn+P2xAw+xGgQMcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe74e376e80>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
