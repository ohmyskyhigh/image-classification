{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff7056d5160>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    normalized = (255 - x) / 255\n",
    "    return x / np.max(x, axis = 0)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    rows = len(x)      \n",
    "    onehot_x = np.zeros([rows,10])\n",
    "\n",
    "    for idx,v in enumerate(x):      \n",
    "        onehot_x[idx][v] = 1\n",
    "    return onehot_x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    a = image_shape[0]\n",
    "    b = image_shape[1]\n",
    "    c = image_shape[2]\n",
    "    return tf.placeholder(tf.float32, shape=(None, a, b, c), name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=[None, n_classes], name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d(x, W, b, strides, p='SAME'):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides[0], strides[1], 1], padding=p)\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, ksize, stride, p='SAME'):\n",
    "    return tf.nn.max_pool(x, ksize=[1, ksize[0], ksize[1], 1], strides=[1, stride[0], stride[1], 1], padding=p)\n",
    "\n",
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of output filters for the convolutional layer (new depth)\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #tensor shape\n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "    shape = shape[3]\n",
    "    weights = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1], shape, conv_num_outputs], mean=0, \n",
    "                                              stddev=0.05))\n",
    "    bias = tf.Variable(tf.truncated_normal([conv_num_outputs], mean=0, stddev=0.05))\n",
    "    conv = conv2d(x_tensor, weights, bias, conv_strides)\n",
    "    conv = maxpool2d(conv, pool_ksize, pool_strides)\n",
    "    return conv\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 9,2])\n",
    "    shape = x_tensor.get_shape().as_list()        # a list: [None, 9, 2]\n",
    "    dim = numpy.prod(shape[1:])            # dim = prod(9,2) = 18\n",
    "    x2 = tf.reshape(x, [-1, dim])           # -1 means \"all\n",
    "    \n",
    "    return tf.contrib.layers.flatten(x_tensor) \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    ###Neurons in a fully connected \n",
    "    ##layer have full connections to all activations in the previous layer, \n",
    "    ##as seen in regular Neural Networks.\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 8, 8, 64)\n",
      "(?, 4, 4, 128)\n",
      "(?, 1, 1, 150)\n",
      "(?, 8, 8, 64)\n",
      "(?, 4, 4, 128)\n",
      "(?, 1, 1, 150)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    conv_num_outputs = 64\n",
    "    conv_ksize = (5, 5)\n",
    "    conv_strides = (2, 2)\n",
    "    pool_ksize = (3, 3)\n",
    "    pool_strides = (2, 2)\n",
    "    # Function Definition from Above:\n",
    "    conv1 = conv2d_maxpool(x, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    print(conv1.get_shape())\n",
    "    conv1 = tf.nn.dropout(conv1, keep_prob)    \n",
    "    conv2 = conv2d_maxpool(conv1, 128, (2, 2), (1, 1), (2, 2), (2, 2))\n",
    "    conv2 = tf.nn.dropout(conv2, keep_prob)\n",
    "    conv3 = conv2d_maxpool(conv2, 150, (4, 4), (4, 4), (1, 1), (1, 1))\n",
    "    print(conv2.get_shape())\n",
    "    print(conv3.get_shape())\n",
    "\n",
    "    \n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    flatten_in = flatten(conv3)\n",
    "    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    num_outputs = 200\n",
    "    # Function Definition from Above:\n",
    "    fullyN = fully_conn(flatten_in, num_outputs)\n",
    "    fullyN = tf.nn.dropout(fullyN, keep_prob)\n",
    "    fullyN = fully_conn(fullyN, 100)\n",
    "\n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    #because cifar-10, maybe\n",
    "    num_outputs = 10\n",
    "    # Function Definition from Above:\n",
    "    out = output(fullyN, num_outputs)\n",
    "    \n",
    "    \n",
    "    # TODO: return output\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    return session.run(optimizer, feed_dict={x: feature_batch,\n",
    "                                             y: label_batch,\n",
    "                                             keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={x: feature_batch,\n",
    "                                       y: label_batch,\n",
    "                                       keep_prob: 1.})\n",
    "    valid_acc = session.run(accuracy, feed_dict={x: valid_features,\n",
    "                                                y: valid_labels,\n",
    "                                                keep_prob: 1.})\n",
    "    print('Loss: {:>10.4f} Accuracy: {:.6f}'.format(loss,valid_acc))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2712 Accuracy: 0.185400\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.0484 Accuracy: 0.292400\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.9197 Accuracy: 0.362800\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.8421 Accuracy: 0.392000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.8308 Accuracy: 0.421600\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.7243 Accuracy: 0.432800\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.6985 Accuracy: 0.443800\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.6333 Accuracy: 0.454800\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.5988 Accuracy: 0.462200\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.6309 Accuracy: 0.463800\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.5672 Accuracy: 0.474000\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.5468 Accuracy: 0.474800\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.4519 Accuracy: 0.514600\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.2908 Accuracy: 0.530000\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.2322 Accuracy: 0.547000\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.2269 Accuracy: 0.545600\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.3043 Accuracy: 0.528200\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.2004 Accuracy: 0.550200\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.1077 Accuracy: 0.553800\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.1413 Accuracy: 0.539600\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.0864 Accuracy: 0.560200\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.1027 Accuracy: 0.553000\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.0525 Accuracy: 0.560400\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     0.9981 Accuracy: 0.578000\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.0131 Accuracy: 0.576400\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     0.9419 Accuracy: 0.580800\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     0.9913 Accuracy: 0.580400\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     0.9965 Accuracy: 0.571600\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     0.9127 Accuracy: 0.568200\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     0.9037 Accuracy: 0.585800\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     0.9039 Accuracy: 0.591200\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     0.9346 Accuracy: 0.577800\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     0.8205 Accuracy: 0.586800\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     0.9412 Accuracy: 0.582400\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     0.8489 Accuracy: 0.594200\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     0.8165 Accuracy: 0.593200\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     0.8948 Accuracy: 0.587400\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     0.8153 Accuracy: 0.593600\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     0.8467 Accuracy: 0.600000\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     0.7961 Accuracy: 0.599400\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     0.7474 Accuracy: 0.608000\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     0.7371 Accuracy: 0.602200\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     0.7513 Accuracy: 0.596800\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     0.7647 Accuracy: 0.593800\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     0.7768 Accuracy: 0.604000\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.6557 Accuracy: 0.611800\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.7278 Accuracy: 0.607800\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.7000 Accuracy: 0.599600\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.6262 Accuracy: 0.614400\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.6527 Accuracy: 0.612800\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.6258 Accuracy: 0.608200\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.6421 Accuracy: 0.608600\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.6169 Accuracy: 0.613000\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.6372 Accuracy: 0.616600\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.5684 Accuracy: 0.611600\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.5574 Accuracy: 0.608000\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.5759 Accuracy: 0.609800\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.6246 Accuracy: 0.607800\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.5627 Accuracy: 0.613200\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.5735 Accuracy: 0.615600\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.5151 Accuracy: 0.611200\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.5422 Accuracy: 0.610800\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.5466 Accuracy: 0.612200\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.5314 Accuracy: 0.599400\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.4768 Accuracy: 0.621800\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.4693 Accuracy: 0.616000\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.4984 Accuracy: 0.606800\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.4555 Accuracy: 0.614400\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.4483 Accuracy: 0.614200\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.4514 Accuracy: 0.604600\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.4044 Accuracy: 0.611400\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.3682 Accuracy: 0.613400\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.3916 Accuracy: 0.612400\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.3690 Accuracy: 0.623400\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.4400 Accuracy: 0.610200\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.4110 Accuracy: 0.619400\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.4120 Accuracy: 0.614400\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.3753 Accuracy: 0.615800\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.4148 Accuracy: 0.614800\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.3112 Accuracy: 0.623400\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.4460 Accuracy: 0.612200\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.3941 Accuracy: 0.617000\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.3997 Accuracy: 0.625000\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.3923 Accuracy: 0.602800\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.3058 Accuracy: 0.617000\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.2872 Accuracy: 0.614000\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.2910 Accuracy: 0.619200\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.2813 Accuracy: 0.618800\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.3007 Accuracy: 0.617000\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.2905 Accuracy: 0.617000\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.2991 Accuracy: 0.613200\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.2687 Accuracy: 0.623000\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.2478 Accuracy: 0.622000\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.2827 Accuracy: 0.610000\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.2242 Accuracy: 0.610000\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.2942 Accuracy: 0.597400\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.2800 Accuracy: 0.610000\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.2860 Accuracy: 0.609000\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.2203 Accuracy: 0.612600\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.2272 Accuracy: 0.615800\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.3019 Accuracy: 0.098800\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.2883 Accuracy: 0.134400\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     2.2228 Accuracy: 0.267000\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     1.9712 Accuracy: 0.330800\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     1.8926 Accuracy: 0.370600\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     1.8544 Accuracy: 0.407600\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     1.8256 Accuracy: 0.413000\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     1.6855 Accuracy: 0.387000\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     1.5424 Accuracy: 0.460000\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     1.5741 Accuracy: 0.466000\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.5696 Accuracy: 0.494400\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     1.3870 Accuracy: 0.496600\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     1.2905 Accuracy: 0.494000\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     1.3778 Accuracy: 0.519600\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     1.4053 Accuracy: 0.515200\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.4127 Accuracy: 0.526200\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     1.2890 Accuracy: 0.517200\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     1.1959 Accuracy: 0.516400\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     1.2958 Accuracy: 0.549200\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     1.3535 Accuracy: 0.544200\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.3073 Accuracy: 0.559000\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     1.2639 Accuracy: 0.551000\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     1.1548 Accuracy: 0.563000\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     1.2079 Accuracy: 0.574800\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     1.2696 Accuracy: 0.554600\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.1724 Accuracy: 0.578000\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     1.1587 Accuracy: 0.573800\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     1.0907 Accuracy: 0.579000\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     1.1704 Accuracy: 0.579000\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     1.1940 Accuracy: 0.559200\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.0707 Accuracy: 0.599400\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     1.1370 Accuracy: 0.600200\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     1.1213 Accuracy: 0.583400\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     1.1348 Accuracy: 0.585600\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     1.1538 Accuracy: 0.579800\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.0540 Accuracy: 0.604000\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     1.0636 Accuracy: 0.606000\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     1.0317 Accuracy: 0.604800\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     1.0497 Accuracy: 0.619400\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     1.0697 Accuracy: 0.601800\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     0.9962 Accuracy: 0.618000\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     1.0899 Accuracy: 0.618600\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     0.9676 Accuracy: 0.625200\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     1.0246 Accuracy: 0.625800\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     1.1555 Accuracy: 0.608000\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     0.9845 Accuracy: 0.630600\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     1.0162 Accuracy: 0.626000\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     0.9432 Accuracy: 0.625400\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     0.9742 Accuracy: 0.640600\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     1.1200 Accuracy: 0.605800\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     0.9155 Accuracy: 0.638400\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     0.9488 Accuracy: 0.637400\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     0.9130 Accuracy: 0.635800\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     0.9400 Accuracy: 0.643000\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     0.9386 Accuracy: 0.631800\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     0.9007 Accuracy: 0.648000\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     0.9972 Accuracy: 0.632000\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     0.8813 Accuracy: 0.636200\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     0.8370 Accuracy: 0.650000\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     0.9299 Accuracy: 0.643200\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     0.8423 Accuracy: 0.651800\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     0.9260 Accuracy: 0.647200\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     0.8183 Accuracy: 0.655200\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     0.8093 Accuracy: 0.654200\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     0.9084 Accuracy: 0.647000\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     0.8310 Accuracy: 0.660200\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     0.9052 Accuracy: 0.646000\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     0.7890 Accuracy: 0.650200\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     0.8387 Accuracy: 0.650800\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     0.8289 Accuracy: 0.660600\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     0.8372 Accuracy: 0.652200\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     0.8830 Accuracy: 0.658400\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     0.7307 Accuracy: 0.663600\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     0.7372 Accuracy: 0.655800\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     0.7996 Accuracy: 0.665200\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     0.8040 Accuracy: 0.659600\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     0.8477 Accuracy: 0.663200\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     0.6646 Accuracy: 0.676200\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     0.7399 Accuracy: 0.671200\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     0.7746 Accuracy: 0.658200\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     0.7869 Accuracy: 0.674200\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     0.8279 Accuracy: 0.665200\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     0.7038 Accuracy: 0.664600\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     0.7431 Accuracy: 0.667200\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     0.7187 Accuracy: 0.669800\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     0.7721 Accuracy: 0.671600\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     0.7767 Accuracy: 0.678000\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     0.6640 Accuracy: 0.677800\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     0.6941 Accuracy: 0.684000\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     0.7894 Accuracy: 0.666400\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     0.8020 Accuracy: 0.660200\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     0.7888 Accuracy: 0.679000\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     0.6781 Accuracy: 0.674600\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     0.6924 Accuracy: 0.679800\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     0.8020 Accuracy: 0.683800\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     0.7851 Accuracy: 0.682000\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     0.7695 Accuracy: 0.690800\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     0.6356 Accuracy: 0.684200\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     0.6460 Accuracy: 0.686600\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     0.7787 Accuracy: 0.672200\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     0.7243 Accuracy: 0.682600\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:     0.8548 Accuracy: 0.677600\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:     0.5785 Accuracy: 0.688000\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:     0.6425 Accuracy: 0.691400\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:     0.7327 Accuracy: 0.677000\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     0.7371 Accuracy: 0.691400\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:     0.7891 Accuracy: 0.688800\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:     0.5660 Accuracy: 0.687600\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:     0.6396 Accuracy: 0.685800\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:     0.6708 Accuracy: 0.678200\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     0.7021 Accuracy: 0.683600\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:     0.7714 Accuracy: 0.674200\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:     0.5395 Accuracy: 0.698600\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:     0.5850 Accuracy: 0.699800\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:     0.6875 Accuracy: 0.679800\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     0.6697 Accuracy: 0.691800\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:     0.7543 Accuracy: 0.696600\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:     0.5558 Accuracy: 0.697200\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:     0.6137 Accuracy: 0.694600\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:     0.6727 Accuracy: 0.684000\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     0.6883 Accuracy: 0.699400\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:     0.7725 Accuracy: 0.682000\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:     0.5790 Accuracy: 0.690600\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:     0.5919 Accuracy: 0.693200\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:     0.6347 Accuracy: 0.703600\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     0.6380 Accuracy: 0.692000\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:     0.7429 Accuracy: 0.696400\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:     0.5265 Accuracy: 0.702400\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:     0.6031 Accuracy: 0.696200\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:     0.6881 Accuracy: 0.684200\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     0.6655 Accuracy: 0.697800\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:     0.7286 Accuracy: 0.699000\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:     0.5121 Accuracy: 0.704200\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:     0.5616 Accuracy: 0.709400\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:     0.6745 Accuracy: 0.692000\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     0.6348 Accuracy: 0.694400\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:     0.7356 Accuracy: 0.695000\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:     0.5314 Accuracy: 0.705400\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:     0.5110 Accuracy: 0.708000\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:     0.6260 Accuracy: 0.692200\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     0.6061 Accuracy: 0.699600\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:     0.7189 Accuracy: 0.696000\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:     0.5043 Accuracy: 0.706600\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:     0.5923 Accuracy: 0.707400\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:     0.6619 Accuracy: 0.703400\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     0.5864 Accuracy: 0.708600\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:     0.7633 Accuracy: 0.700400\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:     0.5558 Accuracy: 0.694400\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:     0.5756 Accuracy: 0.703400\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:     0.5756 Accuracy: 0.706000\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     0.5756 Accuracy: 0.706200\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:     0.7207 Accuracy: 0.692800\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:     0.4805 Accuracy: 0.711000\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:     0.5155 Accuracy: 0.717400\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:     0.5402 Accuracy: 0.708600\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     0.6042 Accuracy: 0.707600\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:     0.6396 Accuracy: 0.708000\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:     0.4956 Accuracy: 0.707600\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:     0.4972 Accuracy: 0.707600\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:     0.5743 Accuracy: 0.702400\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     0.6080 Accuracy: 0.702000\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:     0.6223 Accuracy: 0.707000\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:     0.4857 Accuracy: 0.699200\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:     0.5134 Accuracy: 0.709000\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:     0.5546 Accuracy: 0.710000\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     0.5991 Accuracy: 0.714200\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:     0.6322 Accuracy: 0.705200\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:     0.4414 Accuracy: 0.717000\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:     0.4902 Accuracy: 0.710600\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:     0.5727 Accuracy: 0.704800\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     0.5752 Accuracy: 0.722000\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:     0.7346 Accuracy: 0.713200\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:     0.4844 Accuracy: 0.718400\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:     0.4421 Accuracy: 0.715600\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:     0.5117 Accuracy: 0.714000\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     0.6121 Accuracy: 0.709400\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:     0.6611 Accuracy: 0.704400\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:     0.4614 Accuracy: 0.706800\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:     0.4264 Accuracy: 0.716800\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:     0.5070 Accuracy: 0.703600\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     0.5807 Accuracy: 0.715800\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:     0.6711 Accuracy: 0.719000\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:     0.4699 Accuracy: 0.715200\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:     0.4424 Accuracy: 0.706200\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:     0.5417 Accuracy: 0.711600\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     0.5747 Accuracy: 0.720400\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:     0.6726 Accuracy: 0.709200\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:     0.4426 Accuracy: 0.714600\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:     0.4370 Accuracy: 0.718400\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:     0.4907 Accuracy: 0.705400\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     0.5743 Accuracy: 0.713400\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:     0.6540 Accuracy: 0.714800\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:     0.4177 Accuracy: 0.711800\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:     0.4210 Accuracy: 0.719400\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:     0.5659 Accuracy: 0.698400\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     0.5738 Accuracy: 0.722600\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:     0.6149 Accuracy: 0.715800\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:     0.4132 Accuracy: 0.711400\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:     0.4410 Accuracy: 0.719800\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:     0.5204 Accuracy: 0.702200\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     0.5776 Accuracy: 0.717400\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:     0.6449 Accuracy: 0.721000\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:     0.3876 Accuracy: 0.717600\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:     0.3716 Accuracy: 0.719000\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:     0.4717 Accuracy: 0.715400\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     0.5328 Accuracy: 0.717000\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:     0.5823 Accuracy: 0.718600\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:     0.4169 Accuracy: 0.716000\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:     0.3782 Accuracy: 0.724400\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:     0.4729 Accuracy: 0.718400\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     0.5615 Accuracy: 0.724400\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:     0.5720 Accuracy: 0.721600\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:     0.4171 Accuracy: 0.719000\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:     0.4047 Accuracy: 0.723800\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:     0.4692 Accuracy: 0.716200\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     0.5301 Accuracy: 0.718600\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:     0.6152 Accuracy: 0.717400\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:     0.3800 Accuracy: 0.718400\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:     0.3923 Accuracy: 0.716400\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:     0.4968 Accuracy: 0.708200\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     0.5139 Accuracy: 0.721800\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:     0.5758 Accuracy: 0.730400\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:     0.3689 Accuracy: 0.724600\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:     0.4117 Accuracy: 0.724800\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:     0.4539 Accuracy: 0.717400\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.5438 Accuracy: 0.720600\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:     0.5955 Accuracy: 0.719800\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:     0.3969 Accuracy: 0.721000\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:     0.4057 Accuracy: 0.728400\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:     0.4704 Accuracy: 0.709800\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.5239 Accuracy: 0.720400\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:     0.5913 Accuracy: 0.723600\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:     0.3936 Accuracy: 0.727200\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:     0.4156 Accuracy: 0.722400\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:     0.4324 Accuracy: 0.712600\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.5278 Accuracy: 0.717200\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:     0.5799 Accuracy: 0.727800\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:     0.4563 Accuracy: 0.719000\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:     0.3791 Accuracy: 0.726400\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:     0.4419 Accuracy: 0.719400\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.5349 Accuracy: 0.727400\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:     0.5597 Accuracy: 0.723600\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:     0.3668 Accuracy: 0.725400\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:     0.4637 Accuracy: 0.731000\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:     0.4215 Accuracy: 0.724000\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.5073 Accuracy: 0.731400\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:     0.5168 Accuracy: 0.724600\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:     0.3732 Accuracy: 0.721800\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:     0.3501 Accuracy: 0.721000\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:     0.4038 Accuracy: 0.718000\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.4926 Accuracy: 0.720200\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:     0.5763 Accuracy: 0.729800\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:     0.3790 Accuracy: 0.734800\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:     0.2985 Accuracy: 0.734200\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:     0.4590 Accuracy: 0.711000\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.4846 Accuracy: 0.723200\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:     0.5209 Accuracy: 0.724200\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:     0.3361 Accuracy: 0.723600\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:     0.3268 Accuracy: 0.729600\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:     0.4357 Accuracy: 0.716600\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.4841 Accuracy: 0.721200\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:     0.5867 Accuracy: 0.723600\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:     0.3347 Accuracy: 0.724200\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:     0.3558 Accuracy: 0.732600\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:     0.3681 Accuracy: 0.717800\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.4563 Accuracy: 0.733600\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:     0.5045 Accuracy: 0.730800\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:     0.3864 Accuracy: 0.718400\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:     0.3234 Accuracy: 0.726000\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:     0.3907 Accuracy: 0.713000\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.4848 Accuracy: 0.725200\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:     0.5325 Accuracy: 0.719200\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:     0.3437 Accuracy: 0.724600\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:     0.3333 Accuracy: 0.727400\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:     0.4020 Accuracy: 0.729600\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.4452 Accuracy: 0.724800\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:     0.5702 Accuracy: 0.721800\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:     0.3076 Accuracy: 0.724400\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:     0.3280 Accuracy: 0.737400\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:     0.4170 Accuracy: 0.716800\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.3916 Accuracy: 0.734400\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:     0.5397 Accuracy: 0.733600\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:     0.2899 Accuracy: 0.730400\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:     0.2993 Accuracy: 0.734200\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:     0.4034 Accuracy: 0.719000\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.4178 Accuracy: 0.729400\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:     0.5077 Accuracy: 0.733400\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:     0.3112 Accuracy: 0.724000\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:     0.3549 Accuracy: 0.725600\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:     0.3915 Accuracy: 0.721800\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.4227 Accuracy: 0.728800\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:     0.4719 Accuracy: 0.725200\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:     0.3706 Accuracy: 0.727200\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:     0.3388 Accuracy: 0.728600\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:     0.3579 Accuracy: 0.727400\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.4183 Accuracy: 0.730200\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:     0.5157 Accuracy: 0.730600\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:     0.3086 Accuracy: 0.732600\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:     0.3184 Accuracy: 0.732600\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:     0.3349 Accuracy: 0.723600\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.4076 Accuracy: 0.726600\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:     0.5017 Accuracy: 0.732000\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:     0.2912 Accuracy: 0.728400\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:     0.3040 Accuracy: 0.735200\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:     0.3764 Accuracy: 0.720400\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.4518 Accuracy: 0.727800\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:     0.5430 Accuracy: 0.727400\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:     0.2887 Accuracy: 0.732400\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:     0.3002 Accuracy: 0.734800\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:     0.3708 Accuracy: 0.730800\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.4334 Accuracy: 0.737800\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:     0.4940 Accuracy: 0.732000\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:     0.3177 Accuracy: 0.734800\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:     0.2629 Accuracy: 0.737600\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:     0.3605 Accuracy: 0.724600\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.4488 Accuracy: 0.735400\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:     0.4459 Accuracy: 0.720200\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:     0.2768 Accuracy: 0.733600\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:     0.3187 Accuracy: 0.733800\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:     0.3676 Accuracy: 0.729400\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.4349 Accuracy: 0.734800\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:     0.4556 Accuracy: 0.741000\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:     0.2979 Accuracy: 0.733400\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:     0.2734 Accuracy: 0.737800\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss:     0.3354 Accuracy: 0.730400\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.3924 Accuracy: 0.735400\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:     0.4698 Accuracy: 0.729200\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:     0.2978 Accuracy: 0.735000\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:     0.3188 Accuracy: 0.736600\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:     0.3470 Accuracy: 0.725600\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.4413 Accuracy: 0.717800\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:     0.4993 Accuracy: 0.730400\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:     0.3034 Accuracy: 0.733400\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:     0.2880 Accuracy: 0.735600\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:     0.2917 Accuracy: 0.733400\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.4361 Accuracy: 0.725200\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:     0.4810 Accuracy: 0.731600\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:     0.2958 Accuracy: 0.737200\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:     0.2849 Accuracy: 0.740400\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:     0.3632 Accuracy: 0.717200\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.4605 Accuracy: 0.742200\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:     0.4416 Accuracy: 0.736200\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:     0.3015 Accuracy: 0.728600\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:     0.2984 Accuracy: 0.736000\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:     0.3446 Accuracy: 0.720200\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.3910 Accuracy: 0.735800\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:     0.4286 Accuracy: 0.734600\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:     0.3376 Accuracy: 0.728000\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:     0.2933 Accuracy: 0.733200\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:     0.3114 Accuracy: 0.723800\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.3870 Accuracy: 0.735200\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:     0.4518 Accuracy: 0.738600\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:     0.2815 Accuracy: 0.737200\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:     0.3172 Accuracy: 0.735400\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:     0.3275 Accuracy: 0.728000\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.4071 Accuracy: 0.733400\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:     0.4798 Accuracy: 0.735600\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:     0.2902 Accuracy: 0.743000\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:     0.2575 Accuracy: 0.737200\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:     0.3198 Accuracy: 0.737800\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.4443 Accuracy: 0.732600\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:     0.4862 Accuracy: 0.734200\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:     0.3043 Accuracy: 0.736400\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:     0.2827 Accuracy: 0.733600\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:     0.3688 Accuracy: 0.713000\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.3635 Accuracy: 0.736800\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:     0.4649 Accuracy: 0.732800\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:     0.2902 Accuracy: 0.721600\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:     0.2936 Accuracy: 0.738000\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:     0.3345 Accuracy: 0.728800\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.4107 Accuracy: 0.735000\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:     0.4605 Accuracy: 0.739800\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:     0.2944 Accuracy: 0.735200\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:     0.3104 Accuracy: 0.734600\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:     0.3326 Accuracy: 0.723200\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.3969 Accuracy: 0.732600\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:     0.4358 Accuracy: 0.736400\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:     0.3069 Accuracy: 0.738600\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:     0.2623 Accuracy: 0.738000\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:     0.3180 Accuracy: 0.731800\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.4446 Accuracy: 0.735000\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:     0.4174 Accuracy: 0.733600\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:     0.2615 Accuracy: 0.744400\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:     0.2994 Accuracy: 0.726400\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:     0.3041 Accuracy: 0.732000\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.3780 Accuracy: 0.736600\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:     0.4193 Accuracy: 0.736600\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:     0.2618 Accuracy: 0.735600\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:     0.2645 Accuracy: 0.735800\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:     0.3030 Accuracy: 0.729200\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.4004 Accuracy: 0.737600\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:     0.4266 Accuracy: 0.736400\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:     0.2945 Accuracy: 0.741600\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:     0.3094 Accuracy: 0.733000\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:     0.2972 Accuracy: 0.741200\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.3990 Accuracy: 0.737000\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:     0.4063 Accuracy: 0.732400\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:     0.2740 Accuracy: 0.733800\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:     0.2358 Accuracy: 0.735400\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:     0.2815 Accuracy: 0.729200\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.3978 Accuracy: 0.738200\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:     0.4567 Accuracy: 0.733200\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:     0.2498 Accuracy: 0.737000\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:     0.2700 Accuracy: 0.742000\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:     0.2739 Accuracy: 0.729000\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.3739 Accuracy: 0.742400\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:     0.4192 Accuracy: 0.735000\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:     0.2574 Accuracy: 0.727600\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:     0.2623 Accuracy: 0.739000\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:     0.3198 Accuracy: 0.725600\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.3793 Accuracy: 0.736200\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:     0.4451 Accuracy: 0.741400\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:     0.2778 Accuracy: 0.735400\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:     0.2757 Accuracy: 0.740800\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:     0.3207 Accuracy: 0.727200\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.3571 Accuracy: 0.731400\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:     0.4188 Accuracy: 0.739600\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:     0.2723 Accuracy: 0.745000\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:     0.2585 Accuracy: 0.741000\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:     0.3356 Accuracy: 0.724000\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.4037 Accuracy: 0.731400\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:     0.3858 Accuracy: 0.734200\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:     0.2437 Accuracy: 0.730400\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:     0.2621 Accuracy: 0.739800\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:     0.3669 Accuracy: 0.727000\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.3910 Accuracy: 0.731200\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:     0.4457 Accuracy: 0.731800\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:     0.2664 Accuracy: 0.736000\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss:     0.2920 Accuracy: 0.742800\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:     0.2812 Accuracy: 0.737600\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.3715 Accuracy: 0.736000\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:     0.4370 Accuracy: 0.743000\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss:     0.2455 Accuracy: 0.740800\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:     0.2881 Accuracy: 0.738800\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:     0.3235 Accuracy: 0.741600\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.3837 Accuracy: 0.733600\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:     0.4695 Accuracy: 0.738200\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:     0.2739 Accuracy: 0.740800\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:     0.2916 Accuracy: 0.734200\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:     0.3376 Accuracy: 0.735200\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.4110 Accuracy: 0.735600\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:     0.3552 Accuracy: 0.742200\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:     0.2487 Accuracy: 0.735200\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:     0.2405 Accuracy: 0.735400\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:     0.3078 Accuracy: 0.731800\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.3935 Accuracy: 0.734400\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:     0.3573 Accuracy: 0.736000\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:     0.2565 Accuracy: 0.743600\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:     0.2898 Accuracy: 0.742000\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:     0.3289 Accuracy: 0.730600\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.3951 Accuracy: 0.741800\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:     0.3998 Accuracy: 0.739000\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:     0.2514 Accuracy: 0.735000\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:     0.3044 Accuracy: 0.739800\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:     0.2840 Accuracy: 0.736000\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.3874 Accuracy: 0.743000\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:     0.4214 Accuracy: 0.746600\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:     0.2407 Accuracy: 0.740400\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:     0.2551 Accuracy: 0.746400\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:     0.2760 Accuracy: 0.735000\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.3702 Accuracy: 0.742000\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:     0.4024 Accuracy: 0.732400\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:     0.2344 Accuracy: 0.746000\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:     0.2658 Accuracy: 0.740000\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:     0.2926 Accuracy: 0.732600\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.4026 Accuracy: 0.737400\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:     0.3965 Accuracy: 0.737400\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:     0.2439 Accuracy: 0.745400\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:     0.2175 Accuracy: 0.748400\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:     0.2868 Accuracy: 0.729400\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.3865 Accuracy: 0.733800\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:     0.4312 Accuracy: 0.742200\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:     0.2667 Accuracy: 0.738000\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:     0.2523 Accuracy: 0.743000\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:     0.2794 Accuracy: 0.732000\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.3525 Accuracy: 0.745000\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:     0.4492 Accuracy: 0.739400\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:     0.2290 Accuracy: 0.744200\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:     0.2325 Accuracy: 0.742200\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:     0.2942 Accuracy: 0.726800\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.3874 Accuracy: 0.736200\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:     0.4016 Accuracy: 0.737000\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:     0.2490 Accuracy: 0.738800\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:     0.2167 Accuracy: 0.741800\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:     0.2937 Accuracy: 0.737400\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.3414 Accuracy: 0.743000\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:     0.4072 Accuracy: 0.736800\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:     0.2463 Accuracy: 0.740000\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:     0.2209 Accuracy: 0.748400\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:     0.3478 Accuracy: 0.728600\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.3500 Accuracy: 0.742600\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:     0.3557 Accuracy: 0.727800\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:     0.2442 Accuracy: 0.736800\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:     0.2447 Accuracy: 0.754000\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:     0.2874 Accuracy: 0.741400\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.3406 Accuracy: 0.739200\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:     0.3554 Accuracy: 0.732800\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:     0.2256 Accuracy: 0.733400\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:     0.2686 Accuracy: 0.750200\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:     0.2625 Accuracy: 0.743000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.739814082278481\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff6a017e5c0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
